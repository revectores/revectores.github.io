<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Text Classification
 </title>
  
  <link rel="stylesheet" type="text/css" href="http://revectores.com//static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com//static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com//static/css/code.css"> 
  
</head>
<body>

<p><a href="../"><< natural_language_processing</a></p>

<h1 id="text-classification">Text Classification</h1>
<h3 id="basic-concept">1. Basic Concept</h3>
<h5 id="definition"># Definition</h5>
<p>A mapping <span class="math inline">\(h\)</span> from input data <span class="math inline">\(x\)</span> (drawn from instance space <span class="math inline">\(\mathcal X\)</span>) to a label (or labels) <span class="math inline">\(y\)</span> from some enumberable output space <span class="math inline">\(\mathcal Y\)</span>.</p>
<h5 id="classification-of-text-classification"># Classification of Text Classification</h5>
<ul>
<li>Content classification</li>
<li>Emotion classification</li>
<li>Form classification</li>
</ul>
<h5 id="methods"># Methods</h5>
<ul>
<li><p>Hand-coded rules.</p>
<p>pros: high acc</p>
<p>cons: low recall, hard to construct and maintain rule.</p></li>
<li><p><a href="">Supervised learning</a>. Use a training set of <span class="math inline">\(m\)</span> hand-labeled documents <span class="math inline">\((d_1, c_1), \ldots, (d_m, c_m)\)</span>.</p></li>
<li></li>
</ul>
<h3 id="classification-process">2. Classification Process</h3>
<pre class="mermaid"><code>graph LR;

tp[Text Preprocessing]
tr[Text Representation]
fs[Feature Selection]
model[Model Construction]
tp --&gt; tr --&gt; fs --&gt; model</code></pre>
<p>To represent the document as computer-understandable, we shall express document as the vector in vector space. The distance between vector should indicate the semantic distance.</p>
<p>One classical method of vectorize a document using the <strong>bag of word</strong>, which is simply the counts of words:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>bag_of_word <span class="op">=</span> {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>    <span class="st">&quot;word&quot;</span>: <span class="dv">5</span>,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    <span class="st">&quot;hello&quot;</span>: <span class="dv">2</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>    <span class="co"># ...       </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>}</span></code></pre></div>
<p>Problems of bag of word:</p>
<ul>
<li>Words relation lost. The bag of words just counts the word itself but the context information is not included.</li>
<li>Sparse matrix.</li>
</ul>
<p>The word of bags usually are too large to use, we should do features selection based on some statistical correlation indicators between the feature and type like frequency, mutual information, informaytion gain, <span class="math inline">\(\chi^2\)</span> etc.</p>
<p>Convential machine learning models for text classification:</p>
<ul>
<li><a href="">Naive Bayes</a></li>
<li><a href="">Logical Regression</a></li>
<li><a href="">SVM</a></li>
<li><a href="">CNN</a></li>
</ul>
<h3 id="classification">3. Classification</h3>
<h5 id="naive-bayes-algorithm-for-text-classification"># <a href="">Naive Bayes Algorithm</a> for Text Classification</h5>
<p>Recall Naïve Bayes Algorithm which simply applies the Bayes’ Rule to compute all the inverse probability and pick the maxmium one as prediction: <span class="math display">\[
\hat y = \arg\max_{y\in \mathcal Y} P(Y|X)
\]</span> where the inverse probability is computed as <span class="math display">\[
P(Y|X) = \cfrac{P(XY)}{P(X)} = \cfrac{P(Y=y)P(X=x|Y=y)}{\displaystyle\sum_{y\in \mathcal Y}P(Y=y)P(X=x|Y=y)}
\]</span> where terms <span class="math inline">\(P(Y=y), P(X=x|y=y)\)</span> are computed(learned) from training test.</p>
<p>In the context of document classification that vectorizing document by , the parameters are interpreted as:</p>
<ul>
<li><span class="math inline">\(Y\)</span>: The labels set.</li>
<li><span class="math inline">\(X\)</span>: The set of words.</li>
</ul>
<p>We implement Naive Bayes <a href="text_classification_naive_bayes/text_classification_naive_bayes.py">text_classification_naive_bayes.py</a>.</p>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>