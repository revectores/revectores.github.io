<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Support Vector Machine
 </title>
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/code.css"> 
</head>
<body>

<p><a href="../"><< machine_learning</a></p>

<h1 id="support-vector-machine">Support Vector Machine</h1>
<p><span class="math display">\[
\newcommand{\b}{\boldsymbol}
\newcommand{\bw}{\b w}
\newcommand{\bx}{\b x}
\newcommand{\d}{\text{d}}
\newcommand{\dx}{\d x}
\newcommand{\dy}{\d y}
\newcommand{\T}{\text{T}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\]</span></p>
<h3 id="section">1.</h3>
<p>For the training set <span class="math inline">\(D = \{ (\b x_1, y_1), (\b x_2, y_2), ..., (\b x_m, y_m) \}, y_i = \pm 1\)</span>, the primitive idea of SVM is to find the hyperplane that seperate all the points in both sides, and the hyperplane should be place in the “middle” of all points. That is, maximize the distances from the line to the closest two points in both types. By doing this we wish to maximzie the generalization performance.</p>
<p>The hyperplane is denoted by <span class="math inline">\((\bw, b)\)</span>, where: <span class="math display">\[
\bw^\T \bx + b = 0
\]</span> For any point <span class="math inline">\(\bx_0\)</span> in sample space, the distance from <span class="math inline">\(\bx_0\)</span> to <span class="math inline">\((\bw, b)\)</span> can be calculated as <span class="math display">\[
r = \frac{\bw^T\bx + b}{\norm \bw}
\]</span></p>
<p>empirical risk minimization. <span class="math display">\[
R_{emp}(\theta) = \frac{1}{N}\sum_{i=1}^N L(y_i, f(x_i;\theta))
\]</span> Loss function:</p>
<ul>
<li>quadratic <span class="math inline">\(L(y,x,\theta) = \dfrac{1}{2}(y-f(x;\theta))^2\)</span></li>
<li>linear <span class="math inline">\(L(y,x,\theta) = |y-f(x;\theta)|\)</span></li>
<li>binary $$</li>
</ul>
<p>true risk function <span class="math display">\[
R(\theta) = E_P\{L(x, y, \theta)\} = \int_{X\times Y}P(x,y)L(x,y,\theta)\dx\dy\in[0,1]
\]</span> true risk is different with the empirical risk, since its impossible to sample infinitely (which is required by the true risk function).</p>
<p>maximize the margin as the best linear classifiers, to</p>
<ul>
<li></li>
</ul>
<p>the linear classifier attached the “margin points” is named as “support vector”</p>
<ul>
<li><p>classification hyperplane <span class="math display">\[
  w^Tx - b = 0
  \]</span></p></li>
<li><p>positive margin hyperplane</p></li>
<li><p>negative margin hyperplane</p></li>
<li><p>margin between <span class="math inline">\(H+\)</span> adn <span class="math inline">\(H-\)</span>: <span class="math inline">\(\frac{}{}\)</span></p></li>
</ul>
<p>dual problem of the SVM <span class="math display">\[
f(x:\vec\alpha) = \sum_i^N \alpha_i y_i 
\]</span></p>
<p>Langrange method, form conversion: <span class="math display">\[
L_p = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i[y_i(w^T x_i+b)-1] \\
w = \sum_i \alpha_i y_i x_i
\]</span></p>
<p><span class="math display">\[
\max_\alpha \alpha_i - \frac{1}
\]</span></p>
<p>primal problem: solving a variable <span class="math inline">\(w\in\mathbb{R}^d\)</span>. dual problem: solving a variable <span class="math inline">\(\alpha\in\mathbb{R}^N\)</span>. sometimes <span class="math inline">\(N\ll d\)</span>, by dualing we can simplify the computation.</p>
<p><span class="math inline">\(\alpha\)</span> is often sparse with few non-zero elements.</p>
<h3 id="linearly-inseparable-svm">Linearly Inseparable SVM</h3>
<p><span class="math display">\[
y_i(x_i - b) \ge 1 - \
\]</span></p>
<h3 id="nonlinear-svm-kernelization">Nonlinear SVM: Kernelization</h3>
<p>mapping the data to higher-dimensional space. the linear inseparable in lower-dimensional can be linear separable in higher dimensional.</p>
<p>quadratic polynomial <span class="math display">\[
\varphi:
\begin{bmatrix}
x_1 \\ x_2
\end{bmatrix}
\rightarrow
\begin{bmatrix}
x_1^2 \\ x_2^2 \\ \sqrt{2}x_1x_2
\end{bmatrix}
\]</span></p>
<p>kernel function</p>
<p>Define the <span class="math inline">\(\mathcal{X}\)</span> is the input space, and <span class="math inline">\(\kappa(\cdot, \cdot)\)</span> is the symmetric function defined at <span class="math inline">\(\mathcal {X}\times\mathcal{X}\)</span>, <span class="math inline">\(\kappa\)</span> is the kernel function if for arbitrary data <span class="math inline">\(D=\{x_1, x_2, ..., x_m\}\)</span>, the <strong>kernel matrix</strong> <span class="math inline">\(K\)</span> is always positive semidefinite: <span class="math display">\[
K =
\begin{bmatrix}
\kappa(x_1, x_1) &amp; \cdots &amp; \kappa(x_1, x_j) &amp; \cdots &amp; \kappa(x_1, x_m) \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\kappa(x_i, x_1) &amp; \cdots &amp; \kappa(x_i, x_j) &amp; \cdots &amp; \kappa(x_i, x_m) \\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
\kappa(x_m, x_1) &amp; \cdots &amp; \kappa(x_m, x_j) &amp; \cdots &amp; \kappa(x_m, x_m) \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
k(x, y) = \varphi(x)^T \varphi(y) = (x^Ty)^2
\]</span> linear kernel <span class="math display">\[
k(x,y) = x^Ty
\]</span> Radial Basis Function</p>
<p>http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/ <span class="math display">\[
L = \frac{1}{N}\sum_i\sum_{j\neq y_i} \max(0, f_j-f_{y_i}+1) + \lambda\sum_k\sum_l W^2_{k,l}
\]</span> the first item is data loss, and the second item is the regularization loss.</p>
<p>&lt;!–&gt;Non-zero elements correspond to support ve&lt;–&gt;</p>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>