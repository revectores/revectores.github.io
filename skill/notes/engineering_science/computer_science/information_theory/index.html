<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Information Theory
 </title>
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/code.css"> 
</head>
<body>

<p><a href="../"><< computer_science</a></p>

<h1 id="information-theory">Information Theory</h1>
<p><span class="math display">\[
\newcommand{\X}{\mathcal X}
\newcommand{\Y}{\mathcal Y}
\]</span></p>
<h5 id="information-entropy"># Information Entropy</h5>
<p>For the random variable <span class="math inline">\(X\)</span> with its probability density function <span class="math inline">\(p(x)\)</span>, we define the <strong>information entropy</strong> of <span class="math inline">\(X\)</span> as <span class="math display">\[
H(X) = -\sum_x p(x)\log_a p(x)
\]</span> The base <span class="math inline">\(a\)</span> defines the unit of information entropy, we use <span class="math inline">\(a=2\)</span> for computer science in practice, where the unit is <strong>bit</strong>, theoretically we use <span class="math inline">\(a=\text{e}\)</span>, where the unit is <strong>nat</strong>. The physical significance of information entropy is the average uncertainty of the random variable.</p>
<h5 id="mutual-information"># Mutual Information</h5>
<p>The <strong>mutual information(MI)</strong> or <strong>transinformation</strong> is a measure of the mutual dependence between the two variables. More specifically, it quantifies the “amount of information” obtained about one random variable through observing the other random variable.</p>
<p>The mutual information of two jointly discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is calculated as a double sum: <span class="math display">\[
I(X;Y) = \sum_{y\in\Y}\sum_{x\in\X} p_{(X, Y)}(x, y) \log \left( \frac{p_{X, Y}(x, y)}{p_X(x)p_Y(y)} \right)
\]</span> where the <span class="math inline">\(p_{(X, Y)}\)</span> is the joint probability mass function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(p_X\)</span> and <span class="math inline">\(p_Y\)</span> are the marginal probability mass functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> respectively.</p>
<p><span class="math display">\[
\begin{align}
I(X; Y)
&amp;= \sum_{x,y} p(x, y)\log\frac{p(x, y)}{p(x)p(y)} \\
&amp;= \sum p(x, y)\log \frac{p(x|y)}{p(x)} \\
&amp;= -\sum p(x, y)\log p(x) + \sum p(x, y)\log p(x|y) \\
&amp;= -\sum p(x)\log p(x) - \left(-\sum_{x,y}p(x, y)\log p(x|y)\right) \\
&amp;= H(X) - H(X|Y)
\end{align}
\]</span></p>
<p><span class="math display">\[
I(X;Y) = I(Y;X) = H(X)-H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y) \\
I(X;X) = H(X)
\]</span></p>
<p><span class="math display">\[
\begin{align}
H(X_1, X_2, \cdots, X_n)
&amp;= -\sum p(x1, x_2, \cdots, x_n)\log p(x_1, x_2, \cdots, x_n) \\
&amp;= -\sum p(x1, x_2, \cdots, x_n)\log \prod_{i=1}^n p(x_i | x_{i-1}, \cdots, x_1)
\end{align}
\]</span></p>
<p><span class="math display">\[
D(p(x, y) \| q(x, y)) = D(p(x)\| q(x)) + D(p(y|x) \| q(y|x))
\]</span></p>
<p><span class="math display">\[
\begin{align}
D(p(x, y) \| q(x, y))
&amp;= \sum_x\sum_y p(x, y)\log \frac{p(x,y)}{q(x, y)} \\
&amp;= \sum_x\sum_y p(x, y)\log \frac{p(x)p(y|x)}{q(x)q(y|x)} \\
&amp;= \sum_x\sum_y p(x, y)\log \frac{p(x)}{q(x)} + \sum_x\sum_yp(x, y)\log \frac{p(y|x)}{q(y|x)} \\
&amp;= D(p(x)\| q(x)) + D(p(y|x) \| q(y|x))
\end{align}
\]</span></p>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>