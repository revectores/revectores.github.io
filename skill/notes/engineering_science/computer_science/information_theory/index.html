<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Information Theory
 </title>
  
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/code.css"> 
  
</head>
<body>

<p><a href="../"><< computer_science</a></p>

<h1 id="information-theory">Information Theory</h1>
<h5 id="information-entropy"># Information Entropy</h5>
<p>For the random variable <span class="math inline">\(X\)</span> with its probability density function <span class="math inline">\(p(x)\)</span>, we define the <strong>information entropy</strong> of <span class="math inline">\(X\)</span> as <span class="math display">\[
H(X) = -\sum_x p(x)\log_a p(x)
\]</span> The base <span class="math inline">\(a\)</span> defines the unit of information entropy, we use <span class="math inline">\(a=2\)</span> for computer science in practice, where the unit is <strong>bit</strong>, theoretically we use <span class="math inline">\(a=\text{e}\)</span>, where the unit is <strong>nat</strong>. The physical significance of information entropy is the average uncertainty of the random variable.</p>
<p><span class="math display">\[
\begin{align}
I(X; Y)
&amp;= \sum_{x,y} p(x, y)\log\frac{p(x, y)}{p(x)p(y)} \\
&amp;= \sum p(x, y)\log \frac{p(x|y)}{p(x)} \\
&amp;= -\sum p(x, y)\log p(x) + \sum p(x, y)\log p(x|y) \\
&amp;= -\sum p(x)\log p(x) - \left(-\sum_{x,y}p(x, y)\log p(x|y)\right) \\
&amp;= H(X) - H(X|Y)
\end{align}
\]</span></p>
<p><span class="math display">\[
I(X;Y) = I(Y;X) = H(X)-H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y) \\
I(X;X) = H(X)
\]</span></p>
<p><span class="math display">\[
\begin{align}
H(X_1, X_2, \cdots, X_n)
&amp;= -\sum p(x1, x_2, \cdots, x_n)\log p(x_1, x_2, \cdots, x_n) \\
&amp;= -\sum p(x1, x_2, \cdots, x_n)\log \prod_{i=1}^n p(x_i | x_{i-1}, \cdots, x_1)
\end{align}
\]</span></p>
<p><span class="math display">\[
D(p(x, y) \| q(x, y)) = D(p(x)\| q(x)) + D(p(y|x) \| q(y|x))
\]</span></p>
<p><span class="math display">\[
\begin{align}
D(p(x, y) \| q(x, y))
&amp;= \sum_x\sum_y p(x, y)\log \frac{p(x,y)}{q(x, y)} \\
&amp;= \sum_x\sum_y p(x, y)\log \frac{p(x)p(y|x)}{q(x)q(y|x)} \\
&amp;= \sum_x\sum_y p(x, y)\log \frac{p(x)}{q(x)} + \sum_x\sum_yp(x, y)\log \frac{p(y|x)}{q(y|x)} \\
&amp;= D(p(x)\| q(x)) + D(p(y|x) \| q(y|x))
\end{align}
\]</span></p>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>