<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Linear Models
 </title>
  
  <link rel="stylesheet" type="text/css" href="http://revectores.com//static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com//static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com//static/css/code.css"> 
  
</head>
<body>

<p><a href="../"><< sklearn</a></p>

<h1 id="linear-models">Linear Models</h1>
<p>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if <span class="math inline">\(\hat y\)</span> as the predicted value. <span class="math display">\[
\hat y(w,x) = w_0 + \sum_{i=1}^n w_i x_i = w_0 + w_1x_1 + \cdots + w_nx_n
\]</span> Across the module, we designate the vector <span class="math inline">\(w=(w_1, \ldots, w_n)\)</span> as <code>coef_</code> and <span class="math inline">\(w_0\)</span> as <code>intercept_</code>.</p>
<h5 id="ordinary-least-squares"># Ordinary Least Squares</h5>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"><code>LinearRegression</code></a> fits a linear model with coefficients <span class="math inline">\(w=(w_1, \ldots, w_p)\)</span> to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form: <span class="math display">\[
\min_w \|Xw-y\|^2_2
\]</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"><code>LinearRegression</code></a> will take in its <code>fit</code> method arrays X, y and will store the coefficients of the linear model in its <code>coef_</code> member:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>reg <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>reg.fit([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">2</span>]], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="bu">print</span>(<span class="bu">type</span>(reg.coef_), reg.coef_, reg.intercept_)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="co"># &lt;class &#39;numpy.ndarray&#39;&gt; [0.5 0.5] 1.1102230246251565e-16</span></span></code></pre></div>
<p>Generally, the method <code>predict</code> could be applied.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="bu">print</span>(reg.predict([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">5</span>, <span class="dv">5</span>]]))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="co"># [1.11022302e-16 5.00000000e+00]</span></span></code></pre></div>
<p>The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix <span class="math inline">\(X\)</span> have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes <strong>highly sensitive to random errors</strong> in the observed target, producing a large variance. This situation of <a href="">multicollinearity</a> can arise, for example, when data are collected without an experimental design.</p>
<p>The least squares solution is computed using the singular value decomposition of <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is a matrix of shape <code>(n_samples, n_features)</code>, this method has a cost of <span class="math inline">\(O(n_{\text{samples}} n_{\text{features}}^2)\)</span>, assuming that <span class="math inline">\(n_{\text{samples}} \geq n_{\text{features}}\)</span>.</p>
<h5 id="ridge-regression-and-classification"># Ridge Regression and Classification</h5>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge"><code>Ridge</code></a> regression addresses some of the problems of <a href="https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares">Ordinary Least Squares</a> by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares: <span class="math display">\[
\min_{w} || X w - y ||_2^2 + \alpha ||w||_2^2
\]</span> The complexity parameter <span class="math inline">\(\alpha&gt;0\)</span> controls the amount of shrinkage: the larger the value of , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>reg <span class="op">=</span> linear_model.Ridge(alpha<span class="op">=</span><span class="fl">.5</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>reg.fit([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]], [<span class="dv">0</span>, <span class="fl">.1</span>, <span class="dv">1</span>])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="bu">print</span>(reg.coef_, reg.intercept_)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="co"># [0.34545455 0.34545455] 0.13636363636363638</span></span></code></pre></div>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>