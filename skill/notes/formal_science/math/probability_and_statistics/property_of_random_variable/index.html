<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Property of Random Variable
 </title>
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/code.css"> 
</head>
<body>

<p><a href="../"><< probability_and_statistics</a></p>

<h1 id="property-of-random-variable">Property of Random Variable</h1>
<p><span class="math display">\[
\newcommand{\d}{\text{d}}
\newcommand{\dx}{\d x}
\newcommand{\dy}{\d y}
\newcommand{\dt}{\d t}
\newcommand{\comb}{\text{C}}
\newcommand{\E}{\text{E}}
\newcommand{\D}{\text{D}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\euler}{\text{e}}
\]</span></p>
<h3 id="mathematical-expectation">Mathematical Expectation</h3>
<h5 id="definition"># Definition</h5>
<p>For the discrete variable <span class="math inline">\(X\)</span> with distribution is <span class="math inline">\(P(X=x_i) = p_i\)</span>, if the series <span class="math inline">\(\displaystyle\sum |x_i|p_i\)</span> absoluately converges, i.e. <span class="math inline">\(\displaystyle\sum x_ip_i &lt; \infty\)</span>, then we define the mathematical expectation of <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[
\E X = \sum x_i p_i
\]</span></p>
<p>For the continuous variable <span class="math inline">\(X\)</span> with the probability density function <span class="math inline">\(p(x)\)</span>, if the integral <span class="math inline">\(\displaystyle\int_{-\infty}^{\infty} xp(x) \dx\)</span> absolutely converges, i.e. <span class="math inline">\(\displaystyle\int_{-\infty}^{\infty} |x| p(x)~\dx \lt \infty\)</span>, then we define the mathematical expection of <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[
\E X = \int_{-\infty}^{\infty} xp(x) \dx
\]</span></p>
<p>Apparently, due to restriction of absolutely converge, not all the distributions have well-defined mathematical expectation, take an example, for the famous Cauchy Distribution <span class="math inline">\(p(x) = \dfrac{1}{\pi(1+x^2)}\)</span>, whose expectation integral can be computed as</p>
<p><span class="math display">\[
\begin{align}
I = \int_{-\infty}^{\infty} \dfrac{|x|}{\pi(1+x^2)} \dx
= \dfrac{2}{\pi} \int_{0}^{\infty} \dfrac{|x|}{(1+x^2)} \dx
\ge \dfrac{2}{\pi} \int_{0}^{M} \dfrac{x}{(1+x^2)} \dx
= \dfrac{1}{\pi} \ln(1+M^2)
\end{align}
\]</span></p>
<p>That is, <span class="math inline">\(I \rightarrow \infty\)</span>, which disobeys the condition of definition of expectation.</p>
<h5 id="property"># Property</h5>
<p>The most important property of expectation is linearity:</p>
<p><span class="math display">\[
\E(aX+bY) = a\E X + b\E Y
\]</span></p>
<p>which is obvious due to the linearity of integration itself. Specially, the expectation of constant function is the constant itself, i.e. <span class="math inline">\(\E c = c\)</span>.</p>
<p>Based on this, the expectation combination of functions can be devide-and-conquer by the linear decomposition.</p>
<h5 id="expectation-for-variable-function"># Expectation for Variable Function</h5>
<p>Variable <span class="math inline">\(X\)</span> with its distribution and function <span class="math inline">\(Y=f(X)\)</span>, we don’t have to compute the distribution of <span class="math inline">\(Y\)</span> but use the following formula to compute its expectation(the proof is omitted).</p>
<p><span class="math display">\[
EY = E(f(X)) =
\left\{\begin{array}{ll}\begin{align}
&amp; \sum_{k=1}^{\infty}f(x_k)p_k \\
&amp; \int_{-\infty}^{\infty}f(x_k)p_k\dx
\end{align}\end{array}\right.
\]</span></p>
<p>definition of expectation for two-dimension variable</p>
<p>Two-dimension variable function</p>
<p><span class="math display">\[
EY = E(f(X)) =
\left\{\begin{array}{ll}\begin{align}
&amp; \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}f(x_i, y_j)p_{ij} \\
&amp; \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x_i, y_j)p_{ij}\dx\dy
\end{align}\end{array}\right.
\]</span></p>
<h3 id="variance">Variance</h3>
<h5 id="definition-1"># Definition</h5>
<p>Define <span class="math inline">\(\E(X-\E X)^2\)</span> as the <strong>variance</strong> of variable <span class="math inline">\(X\)</span> if the expectation exists (converges), denoted as <span class="math inline">\(\Var X\)</span> or <span class="math inline">\(\D X\)</span>. Define <span class="math inline">\(\sigma_X = \sigma(X) = \sqrt{\D X}\)</span> as the standard deviation of <span class="math inline">\(X\)</span>.</p>
<h5 id="property-1"># Property</h5>
<ol type="1">
<li><p><span class="math inline">\(\D X \ge 0\)</span></p></li>
<li><p><span class="math inline">\(\D c = 0\)</span></p></li>
<li><p><span class="math inline">\(\D(aX+b) = a^2\D X\)</span></p></li>
<li><p><span class="math inline">\(\D X = \E X^2 - (\E X)^2\)</span></p></li>
<li><p><span class="math inline">\(\D(X\pm Y) = \D X + \D Y \pm \Cov(X, Y)\)</span></p>
<p>As we’ll prove in the property of convariance, the convariance for two independant variables are zero, hence<span class="math inline">\(\D(X\pm Y) = \D X \pm \D Y\)</span> holds for independant variables <span class="math inline">\(X, Y\)</span>.</p></li>
</ol>
<p>The variance of standard normal distribution <span class="math inline">\(N(0, 1)\)</span>:</p>
<p><span class="math display">\[
E(X^2) = x^2\varphi(x)\dx = 1
\]</span></p>
<p>The variance of <span class="math inline">\(N(0, \sigma^2)\)</span> is <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Standardization of random variable:</p>
<p><span class="math display">\[
Y = \frac{X-EX}{\sqrt{\Var{X}}}
\]</span></p>
<p>Now we have <span class="math inline">\(\E Y = 0, \Var Y = 1\)</span></p>
<h3 id="property-of-special-distribution">Property of Special Distribution</h3>
<p>Here we compute the expectation and variance of several special distribution: binomial distribution, exponent distribution, and the standard normal distribution.</p>
<ol type="1">
<li>Binomial distribution <span class="math inline">\(P(X=k) = \comb_{n}^{k} p^k(1-p)^{n-k}\)</span></li>
</ol>
<p><span class="math display">\[
\begin{align}
\E X
&amp;= \sum k \comb_{n}^{k} p^k(1-p)^{n-k} \\
&amp;= \sum k \cdot \dfrac{n!}{k!(n-k)!} p^k(1-p)^{n-k} \\
&amp;= np\sum \dfrac{(n-1)!}{(k-1)!(n-1-(k-1))!} p^{k-1}(1-p)^{n-1-(k-1)} \\
&amp;= np
\end{align}
\]</span></p>
<p>==TODO: Find out why the last equality can be reduced by the regularity of binomial distribution.==</p>
<ol start="2" type="1">
<li>Exponent distribution <span class="math inline">\(Exp(\lambda)\)</span>.</li>
</ol>
<p><span class="math display">\[
\E~Exp(\lambda) = \int_{0}^{\infty} x\lambda \euler^{\lambda x} \dx = \dfrac{1}{\lambda}
\]</span></p>
<ol start="3" type="1">
<li>Standard normal distribution</li>
</ol>
<p><span class="math display">\[
\E \varphi(x) = \int_{-\infty}^{\infty} x\phi \dx = 0 
\]</span></p>
<p>Furthermore, with the linear property of expectation applied, for the general normal distribution, <span class="math inline">\(\E X = \E(\sigma X&#39; + \mu) = \sigma \E X + \mu = \mu\)</span>.</p>
<p>We list the result following for the future reference:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">distribution</th>
<th style="text-align: center;">expectation</th>
<th style="text-align: center;">Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(b(n, p) = \comb_n^{k}p^k(1-p)^{n-k}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(np\)</span></td>
<td style="text-align: center;"><span class="math inline">\(np(1-p)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(P(\lambda) = \dfrac{\lambda^k}{k!}\euler^{-\lambda}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\lambda\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\lambda\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(U(a, b)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\dfrac{a+b}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\dfrac{(b-a)^2}{12}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(N(\mu, \sigma^2)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(Ga(\alpha, \lambda)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\dfrac{\alpha}{\lambda}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\dfrac{\alpha}{\lambda^2}\)</span></td>
</tr>
</tbody>
</table>
<h3 id="property-related-inequality">Property Related inequality</h3>
<h5 id="markovs-inequality"># Markov’s Inequality</h5>
<p>For any positive random variable <span class="math inline">\(X&gt;0\)</span> and positive number <span class="math inline">\(b&gt;0\)</span>, we have <span class="math display">\[
P(X\ge a) \le \frac{\E X}{a}
\]</span></p>
<blockquote>
<p>Proof. based on the definition of expectation, <span class="math display">\[
\E X
= \int_{-\infty}^{\infty} xp(x) \dx
= \int_{0}^{\infty} xp(x) \dx
\ge \int_{a}^{\infty} xp(x) \dx
\ge \int_{a}^{\infty} ap(x) \dx
= a\int_{a}^{\infty} p(x) \dx
= a \Pr(X\ge a)
\]</span></p>
</blockquote>
<p>As an interpretation of Markov’s inequation, we consider the distribution of income: assuming no income is negative, Markov’s inequality shows that, no more than 1/5 of the population can have more than 5 times the average income.</p>
<h5 id="chebyshevs-inequality"># Chebyshev’s Inequality</h5>
<p>Chebyshev’s inequality shows the relationships between expectation and variance. In summary, it shows the facts: the variance limits the range of variation of <span class="math inline">\(X\)</span>. For the smaller <span class="math inline">\(\D X\)</span>, <span class="math inline">\(X\)</span> will keep closer to expectation <span class="math inline">\(\E X\)</span>: <span class="math display">\[
P\{ |X-\E X| \ge b \} \le \frac{\D X}{b^2}
\]</span></p>
<h3 id="moment">Moment</h3>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>