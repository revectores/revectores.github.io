<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Linear Equations Solving
 </title>
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/code.css"> 
</head>
<body>

<p><a href="../"><< numerical_analysis</a></p>

<h1 id="linear-equations-solving">Linear Equations Solving</h1>
<p><span class="math display">\[
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\b}{\boldsymbol}
\newcommand{\bx}{\b x}
\newcommand{\by}{\b y}
\newcommand{\bb}{\b b}
\newcommand{\bg}{\b g}
\newcommand{\w}{\widetilde}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\o}{\overline}
\]</span></p>
<p>We’ve kown that the equations with <span class="math inline">\(n\)</span>-variables <span class="math display">\[
\left\{\begin{array}{ll}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1  \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2  \\
\cdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n  \\
\end{array}\right.
\]</span></p>
<p>can be written in matrix form <span class="math inline">\(A\bx = \bb\)</span> <span class="math display">\[
\begin{bmatrix}
a_{11} &amp; a_{12} &amp;\cdots&amp; a_{1n}  \\
a_{21} &amp; a_{22} &amp;\cdots&amp; a_{2n}  \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp;\cdots&amp; a_{nn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n \\
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is <strong>coefficient matrix</strong>, <span class="math inline">\(\bb\)</span> is <strong>constant vector</strong>, and <span class="math inline">\(\bx\)</span> is <strong>solution vector</strong>.</p>
<p><a href="">Cramer’s rule</a> guranttes that for matrix <span class="math inline">\(A\)</span> with <span class="math inline">\(\det A \neq 0\)</span>, the solution must exist and is unique: <span class="math display">\[
x_i = \frac{D_i}{D}, \quad i = 1, 2, \ldots, n
\]</span></p>
<p>where <span class="math inline">\(D_i\)</span> can be generated by replacing the <span class="math inline">\(i\)</span>-th column of coefficient matrix <span class="math inline">\(A\)</span> with constant <span class="math inline">\(\bb\)</span>. Although plays an important role in the thoery of linear equations, the time complexity <span class="math inline">\(O(n!\times n)\)</span> of computing determinant makes it not acceptable to apply Cramer’s rule in large scale of linear systems.</p>
<h5 id="gaussian-elimination"># Gaussian Elimination</h5>
<p><a href="">Gaussian elimination</a>, instead, applies <a href="">elementary matrix transformation</a> to augmented matrix <span class="math inline">\(\begin{bmatrix}A &amp; b\end{bmatrix}^T\)</span> which aims to convert the matrix into an upper triangular matrix, after this transformation, the equation can be easily solved by substituting (down to top). The time complexity for this approach is <span class="math inline">\(O(n^3)\)</span>.</p>
<p>The Gaussian elimination requires <span class="math display">\[
\Delta_k =
\begin{vmatrix}
a_{11} &amp; a_{12} &amp;\cdots&amp; a_{1k}  \\
a_{21} &amp; a_{22} &amp;\cdots&amp; a_{2k}  \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{k1} &amp; a_{k2} &amp;\cdots&amp; a_{kk}
\end{vmatrix}
\neq 0
\]</span> for any <span class="math inline">\(k&lt;n\)</span>. While the solution exists only requires <span class="math inline">\(\det A \neq 0\)</span>. That is, Gaussian elimination can not be applied to some solvable functions with <span class="math inline">\(a_{kk}^{(k-1)} = 0\)</span> during the elimination. Besides, the round-off error will increase for small <span class="math inline">\(a_{kk}^{(k-1)}\)</span>.</p>
<h5 id="gaussian-elimination-with-column-pivoting"># Gaussian Elimination with Column Pivoting</h5>
<p>To overcome the flaw of Gaussian elimination, the column pivoting mechanism for each iteration in elimination is proposed. Formally, for each <span class="math inline">\(k = 1, 2, \ldots, n-1\)</span>, we find the element with maximal absolute value <span class="math inline">\(|a_{m,k}^{(k-1)}|\)</span> within <span class="math inline">\(|a_{k,k}^{k-1}|, |a_{k+1,k}^{k-1}|, \ldots |a_{n,k}^{k-1}|\)</span> then swap row <span class="math inline">\(k, m\)</span> before elimination. With column pivoting applied, we can solve any linear equations with <span class="math inline">\(\det A \neq 0\)</span>. Since the total time complexity of comparison process is <span class="math inline">\(O(n^2)\)</span>, the time complexity Gaussian elimination with column pivoting is still <span class="math inline">\(O(n^3)\)</span>.</p>
<h5 id="gauss-jordan-elimination"># Gauss-Jordan Elimination</h5>
<p>Gaussian elimination execute substitution process right after the matrix is converted to upper triangular matrix, while we can do further elimination to convert it into a diagonal, which is called <strong>Gauss-Jordan elimination</strong>.</p>
<h3 id="matrix-decomposition">2. Matrix Decomposition</h3>
<h5 id="linear-transformation-and-matrix-decomposition"># Linear Transformation and Matrix Decomposition</h5>
<p>An elementary line transformation is equivalent to multiply with a elementary matrix. The process of Gaussian elimination is equivalent to multiple a matrix <span class="math inline">\(T\)</span> to <span class="math inline">\(A\)</span>, results in a upper triangular matrix: <span class="math display">\[
TA = U
\]</span></p>
<p>Multiple <span class="math inline">\(L = T^{-1}\)</span> on both sides: <span class="math display">\[
A = LU
\]</span> where <span class="math display">\[
L = T^{-1} =
\begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\]</span> That is, we devide the matrix <span class="math inline">\(A\)</span> into the multiplication of two matrixes <span class="math inline">\(L\)</span>, <span class="math inline">\(U\)</span>. where <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> are lower and upper triangular matrix respectively. Equivalently, we can also decompose by <span class="math inline">\(A=UL\)</span>.</p>
<p>For form <span class="math inline">\(A=LU\)</span>, if <span class="math inline">\(L\)</span> is contrainted as the unit lower triangle matrix, this is called <strong>Doolittle decomposition</strong>, while for form <span class="math inline">\(A = LU\)</span>, and the <span class="math inline">\(U\)</span> is contrainted as the unit upper triangle matrix,</p>
<p>This composition process is not relavent to <span class="math inline">\(\bb\)</span>, hence if there are multiple equations with same <span class="math inline">\(A\)</span> but different <span class="math inline">\(\bb\)</span>, the composition method reduce the time complexity by cancelling the repetative processing of <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[
A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp;\cdots&amp; a_{1n}  \\
a_{21} &amp; a_{22} &amp;\cdots&amp; a_{2n}  \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp;\cdots&amp; a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp; u_{12} &amp;\cdots &amp; u_{1n}  \\
       &amp; u_{22} &amp;\cdots &amp; u_{2n}  \\
       &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
a_{11} = \sum_{r=1}^n l_{1r}u_{r1} =
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{11} \\
0 \\
\vdots \\
0
\end{bmatrix}
= \sum_{r=1}^1 l_{1r}u_{r1} = u_{11}
\]</span></p>
<p><span class="math display">\[
a_{1j} = \sum_{r=1}^n l_{1r}u_{rj} =
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{1j} \\
u_{2j} \\
\vdots \\
0
\end{bmatrix}
= \sum_{r=1}^1 l_{1r}u_{rj} = u_{1j}
\]</span> Hence <span class="math inline">\(a_{ij} = u_{1j}\)</span>, <span class="math inline">\(j=1,2, \dots, n\)</span>.</p>
<p><span class="math display">\[
\begin{align}
a_{kj}
&amp;= \sum_{r=1}^n l_{kr}u_{rj} =
\begin{bmatrix}
l_{k1} &amp; l_{k2} &amp; \cdots &amp; l_{k,k-1} &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{1j} \\
\vdots \\
u_{jj} \\
0 \\
\vdots \\
0
\end{bmatrix} \\
&amp;= \sum_{r=1}^n l_{kr}u_{rj}
= \sum_{r=1}^{k-1} l_{kr}u_{rj} + u_{kj}
\end{align}
\]</span></p>
<p>For Crout decomposition,</p>
<h5 id="decomposition-of-positiv-definite-matrix"># Decomposition of Positiv Definite Matrix</h5>
<p>For the positive definitive matrix, there exists lower triangle matrix <span class="math inline">\(U\)</span> which satisifies <span class="math inline">\(A = UU^T\)</span>, this is called the square root method, since this computation process involves the computation about square root, in practice we often decompose as form <span class="math inline">\(A = LDL^T\)</span> instaed.</p>
<p>We first apply Doolittle decomposition to matrix and extract the diagonal line of <span class="math inline">\(U\)</span>: <span class="math display">\[
\begin{align}
A = LU
&amp;= \begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp; u_{12} &amp;\cdots &amp; u_{1n}  \\
       &amp; u_{22} &amp;\cdots &amp; u_{2n}  \\
       &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp;        &amp;       &amp;  \\
       &amp; u_{22} &amp;       &amp; \\
       &amp;        &amp;\ddots &amp; \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix}
\begin{bmatrix}
\o u_{11} &amp; \o u_{12} &amp;\cdots &amp; \o u_{1n}  \\
       &amp; \o u_{22} &amp;\cdots &amp; \o u_{2n}  \\
       &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; \o u_{nn}
\end{bmatrix}
\end{align}
\]</span> Since <span class="math inline">\(A\)</span> is symmetric positive definite matrix, we have <span class="math inline">\(u_{ii}&gt;0\)</span>. We can prove <span class="math inline">\(L = \o U\)</span> since <span class="math display">\[
A = LU = LD\o U^T = A^T = \o U(DL^T)
\]</span> That is, <span class="math display">\[
\begin{align}
A = LDL^T
= \begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp;        &amp;       &amp;  \\
       &amp; u_{22} &amp;       &amp; \\
       &amp;        &amp;\ddots &amp; \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix}
\begin{bmatrix}
1      &amp; l_{21} &amp;\cdots &amp; l_{n1}  \\
       &amp; 1      &amp;\cdots &amp; l_{n2}  \\
       &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; 1
\end{bmatrix}
\end{align}
\]</span></p>
<p>Now we have to apply three steps:</p>
<ol type="1">
<li>Solve <span class="math inline">\(LZ = B\)</span>, where <span class="math inline">\(Z = DL^TX\)</span> <span class="math display">\[
z_i = b_i - \sum_{k=1}^{i-1}l_{ij}z_j, \quad i = 1, 2, \ldots, n
\]</span></li>
</ol>
<p><span class="math display">\[
y_i = 
\]</span></p>
<blockquote>
<p><strong>Example</strong>. Solve the equation <span class="math display">\[
\begin{bmatrix}
1 &amp; -1 &amp; 1 \\
-1 &amp; 3 &amp; -2 \\
1 &amp; -2 &amp; 4.5
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
=
\begin{bmatrix}
4 \\
-8 \\
12
\end{bmatrix}
\]</span> by <span class="math inline">\(LDL^T\)</span> decomposition.</p>
</blockquote>
<h5 id="condition-number-of-matrix"># Condition Number of Matrix</h5>
<p>For the non-singular matrix <span class="math inline">\(A\)</span>, we define <span class="math display">\[
\kappa_p(A) = \norm{A}_p \norm{A^{-1}}_p
\]</span> as the <strong>condition number</strong> of matrix <span class="math inline">\(A\)</span>. Although there are multiple norm numbers <span class="math display">\[
\frac{\norm {\delta x}}{\norm x} \le \frac{\kappa_A \dfrac{\norm{\delta A}}{\norm A}}{1 - \kappa_A\dfrac{\norm{\delta A}}{\norm A}}
\]</span> We call the matrix with large <span class="math inline">\(\kappa_A\)</span> ill-conditioned, that is, the small error causes the solution</p>
<h3 id="linear-equation-solving-by-iteration">Linear Equation Solving by Iteration</h3>
<p>Transform the equations <span class="math inline">\(AX = \by\)</span> into <span class="math inline">\(X = MX + \bg\)</span>, for any <span class="math inline">\(X^{(0)}\in R^n\)</span>, if the iteration series <span class="math inline">\(\{X^{(k)}\}\)</span> converges, the limit of iteration series <span class="math inline">\(X^*\)</span> is the solution of equations <span class="math inline">\(AX=\by\)</span>.</p>
<p>If <span class="math inline">\(X^*\)</span> is the solution of equations <span class="math inline">\(AX=\by\)</span>, then <span class="math display">\[
X^* = MX^* + g
\]</span></p>
<p><span class="math display">\[
\begin{align}
X^* - X^{(k+1)}
&amp;= M(X^*-X^{k}) \\
&amp;= M^2(X^*-X^{k-1}) \\
&amp;= \cdots \\
&amp;= M^{k+1}(X^*-X^{(0)})
\end{align}
\]</span></p>
<p><span class="math inline">\(\displaystyle \lim_{k\rightarrow \infin} M^{k} = 0\)</span> if and only if <span class="math inline">\(\rho(M) &lt; 1\)</span>. Hence we define the matrix with spectral radius less than 1 <strong>convergent matrix</strong>.</p>
<p>That is, whether the linear equations converges depends on the property of iteration matrix, regardless of the solution <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(X^{(0)}\)</span>.</p>
<p>By definition, to compute the spectral radius of matrix, we have to compute the</p>
<h5 id="jacobi-iteration"># Jacobi Iteration</h5>
<p><span class="math display">\[
\left\{\begin{array}{ll}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = y_1  \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = y_2  \\
\cdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = y_n  \\
\end{array}\right.
\]</span></p>
<p><span class="math display">\[
\left\{\begin{array}{ll}
x_1 = \dfrac{1}{a_{11}}\left({-a_{12}x_2 - \cdots - a_{1n}x_n + y_1}\right)  \\
x_2 = \dfrac{1}{a_{22}}\left({-a_{21}x_2 - \cdots - a_{2n}x_n + y_2}\right)  \\
\cdots \\
x_n = \dfrac{1}{a_{nn}}\left({-a_{n1}x_2 - \cdots - a_{nn}x_n + y_n}\right)  \\
\end{array}\right.
\]</span></p>
<p>The iteration form <span class="math display">\[
\left\{\begin{array}{ll}
x_1^{(k+1)} = \dfrac{1}{a_{11}}\left({-a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)} + y_1}\right)  \\
x_2^{(k+1)} = \dfrac{1}{a_{22}}\left({-a_{21}x_2^{(k)} - \cdots - a_{2n}x_n^{(k)} + y_2}\right)  \\
\cdots \\
x_n^{(k+1)} = \dfrac{1}{a_{nn}}\left({-a_{n1}x_1^{k} - \cdots - a_{n, n-1}x_n^{(k)} + y_n}\right)  \\
\end{array}\right.
\]</span> Denote <span class="math inline">\(b_{ij} = - a_{ij}/a_{ii}, g_{i} = y_i/a_{ii}\)</span>, the matrix form of Jacobi iteration is $$</p>
<p>$$</p>
<blockquote>
<p>Solve equations with Jacobi iteration: <span class="math display">\[
\left\{\begin{array}{ll}
2x_1 - x_2 - x_3 = -5 \\
x_1 + 5x_2 - x_3 = 8 \\
x_1 + x_2 + 10x_3 = 11
\end{array}\right.
\]</span></p>
</blockquote>
<p>If the matrix <span class="math inline">\(A\)</span> meets one of the condition:</p>
<ul>
<li><span class="math inline">\(|a_{ii}| &gt; \displaystyle \sum_{j=1\\j\neq i}^n |a_ij|,\quad i=1, 2, \ldots, n\)</span>.</li>
<li><span class="math inline">\(|a_{jj}| &gt; \displaystyle \sum_{i=1\\i\neq j}^n |a_ij|,\quad j=1, 2, \ldots, n\)</span></li>
</ul>
<p>The Jacobi iteration must converge.</p>
<blockquote>
<p><strong>Proof</strong>.</p>
</blockquote>
<h5 id="gauss-seidel-iteration"># Gauss-Seidel Iteration</h5>
<p>The Gauss-Seidel iteration use those new values computed in current iteration instead of last one: <span class="math display">\[
\left\{\begin{array}{ll}
x_1^{(k+1)} = \dfrac{1}{a_{11}}\left({-a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)} + y_1}\right)  \\
x_2^{(k+1)} = \dfrac{1}{a_{22}}\left({-a_{21}x_2^{(k+1)} - \cdots - a_{2n}x_n^{(k)} + y_2}\right)  \\
\cdots \\
x_n^{(k+1)} = \dfrac{1}{a_{nn}}\left({-a_{n1}x_1^{(k+1)} - \cdots - a_{n, n-1}x_{n-1}^{(k+1)} + y_n}\right)  \\
\end{array}\right.
\]</span> Denote <span class="math display">\[
D =
\begin{bmatrix}
a_{11}    \\
&amp; a_{22}  \\
&amp;&amp; \ddots \\
&amp;&amp;&amp; a_{nn}
\end{bmatrix},
L =
\begin{bmatrix}
0 \\
a_{21} &amp; 0  \\
\vdots &amp; \vdots &amp;\ddots \\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; 0
\end{bmatrix},
U =
\begin{bmatrix}
0      &amp; a_{12} &amp;\cdots &amp; a_{1n}  \\
       &amp; 0      &amp;\cdots &amp; a_{2n}  \\
       &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; 0
\end{bmatrix}
\]</span> We have <span class="math display">\[
AX = (D+L+U)X = (D+L)X + UX = \by
\]</span> That is, <span class="math display">\[
(D+L)X = -UX + \by
\]</span> hence <span class="math display">\[
X^{(k+1)} = -(D+L)^{-1}UX^{(k)} + (D+L)^{-1}\by
\]</span> Denote <span class="math inline">\(S = -(D+L)^{-1}U, \b f = (D+L)^{-1}\by\)</span>, the Gauss-Seidel iteration can be expressed by <span class="math display">\[
X^{k+1} = SX^{k} + \b f
\]</span></p>
<h3 id="successive-over-relaxation">Successive Over-Relaxation</h3>
<p><span class="math display">\[
\begin{align}
&amp; X^{(k+1)} = X^{(k)} + \omega\Delta X^{(k)} \\
&amp; X^{(k+1)} = X^{(k)} + \omega(X^{(k+1)} - X^{(k)} ) \\
&amp; X^{(k+1)} = (1-\omega)X^{(k)} + \omega\Delta X^{(k+1)} \\
\end{align}
\]</span> Hence <span class="math display">\[
X^{(k+1)} = (1-\omega)X^{(k)} + \omega(\widetilde LX^{(k+1)} + \widetilde UX^{(k)} + \bg)
\]</span></p>
<p><span class="math display">\[
\left\{\begin{array}{ll}
x_1^{(k+1)} = (1-\omega)x_1^{(k)} + \omega(b_{12}x_2^{k} + \cdots + b_{1n}x_n^{(k)} + g_1) \\
x_2^{(k+1)} = (1-\omega)x_2^{(k)} + \omega(b_{21}x_2^{k} + \cdots + b_{2n}x_n^{(k)} + g_2) \\
\cdots \\
x_2^{(k+1)} = (1-\omega)x_n^{(k)} + \omega(b_{n1}x_2^{(k+1)} + \cdots + b_{n, n-1}x_{n-1}^{(k+1)} + g_n) \\
\end{array}\right.
\]</span></p>
<p><span class="math display">\[
\begin{align}
&amp; X^{(k+1)} = (1-\omega)X^{(k)} + \omega(\widetilde LX^{(k+1)} + \widetilde UX^{(k)} + \bg) \\
&amp; (I - \omega \widetilde L)X^{(k+1)} = ((1-\omega)I + \omega \widetilde U)X^{(k)} + \omega\bg \\
&amp; X^{(k+1)} = (I-\omega)
\end{align}
\]</span></p>
<p>The necessary conditions of SOR is <span class="math inline">\(0 &lt; \omega &lt; 2\)</span> .</p>
<p>For succeeive iteration matrix <span class="math inline">\(S_\omega\)</span>, the iteration converges if <span class="math inline">\(\rho(S_\omega) &lt; 1\)</span>, or <span class="math inline">\(\norm{S_\omega}_p &lt; 1\)</span>.</p>
<p>If <span class="math inline">\(A\)</span> is the positive determined matrix, <span class="math inline">\(0 &lt; \omega &lt; 2\)</span></p>
<p>We call the SOR with <span class="math inline">\(0 &lt; \omega &lt; 1\)</span> as</p>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>