<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Linear Equations Solving
 </title>
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/code.css"> 
</head>
<body>

<p><a href="../"><< numerical_analysis</a></p>

<h1 id="linear-equations-solving">Linear Equations Solving</h1>
<p><span class="math display">\[
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\b}{\boldsymbol}
\newcommand{\bx}{\b{x}}
\newcommand{\by}{\b{y}}
\newcommand{\bb}{\b{b}}
\newcommand{\bf}{\b{f}}
\newcommand{\bg}{\b{g}}
\newcommand{\w}{\widetilde}
\newcommand{\wL}{\w L}
\newcommand{\wU}{\w U}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\o}{\overline}
\]</span></p>
<h3 id="linear-equation-solving-by-elimination">1. Linear Equation Solving by Elimination</h3>
<p>We’ve kown that the equations with <span class="math inline">\(n\)</span>-variables <span class="math display">\[
\left\{\begin{array}{ll}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1  \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2  \\
\cdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n  \\
\end{array}\right.
\]</span></p>
<p>can be written in matrix form <span class="math inline">\(A\bx = \bb\)</span> <span class="math display">\[
\begin{bmatrix}
a_{11} &amp; a_{12} &amp;\cdots&amp; a_{1n}  \\
a_{21} &amp; a_{22} &amp;\cdots&amp; a_{2n}  \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp;\cdots&amp; a_{nn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n \\
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is <strong>coefficient matrix</strong>, <span class="math inline">\(\bb\)</span> is <strong>constant vector</strong>, and <span class="math inline">\(\bx\)</span> is <strong>solution vector</strong>.</p>
<p><a href="https://en.wikipedia.org/wiki/Cramer&#39;s_rule">Cramer’s rule</a> guarantees that for coefficient matrix <span class="math inline">\(A\)</span> with <span class="math inline">\(\det A \neq 0\)</span>, the solution exists and is unique: <span class="math display">\[
x_i = \frac{D_i}{D}, \quad i = 1, 2, \ldots, n
\]</span></p>
<p>where <span class="math inline">\(D_i\)</span> can be generated by replacing the <span class="math inline">\(i\)</span>-th column of coefficient matrix <span class="math inline">\(A\)</span> with constant vector <span class="math inline">\(\bb\)</span>.</p>
<p>Although Cramer’s rule plays an important role in thoery of linear equations, the time complexity <span class="math inline">\(O(n!\times n)\)</span> of computing determinant makes it not acceptable to apply Cramer’s rule in large scale of linear systems.</p>
<h5 id="gaussian-elimination"># Gaussian Elimination</h5>
<p><strong>Gaussian elimination</strong>, instead, applies <a href="">elementary row operation</a> to augmented matrix <span class="math inline">\(\begin{bmatrix}A &amp; b\end{bmatrix}^T\)</span>. It converts the matrix into an upper triangular matrix, after these operations, the equation can be easily solved by substituting (bottom-up). The time complexity of this approach is <span class="math inline">\(O(n^3)\)</span>.</p>
<p>The Gaussian elimination requires</p>
<p><span class="math display">\[
\Delta_k =
\begin{vmatrix}
a_{11} &amp; a_{12} &amp;\cdots&amp; a_{1k}  \\
a_{21} &amp; a_{22} &amp;\cdots&amp; a_{2k}  \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{k1} &amp; a_{k2} &amp;\cdots&amp; a_{kk}
\end{vmatrix}
\neq 0
\]</span></p>
<p>for any <span class="math inline">\(k \lt n\)</span>. While the solution exists only requires <span class="math inline">\(\det A \neq 0\)</span>. This indicates that Gaussian elimination cannot be applied to some solvable functions with <span class="math inline">\(a_{kk}^{(k-1)} = 0\)</span> during the elimination. Besides, the round-off error will increase for small <span class="math inline">\(a_{kk}^{(k-1)}\)</span>.</p>
<h5 id="gaussian-elimination-with-column-pivoting"># Gaussian Elimination with Column Pivoting</h5>
<p>To overcome the flaw of Gaussian elimination, the column pivoting operation for each iteration in elimination is proposed. Formally, for each <span class="math inline">\(k = 1, 2, \ldots, n-1\)</span>, we search for the element with maximal absolute value <span class="math inline">\(|a_{m,k}^{(k-1)}|\)</span> within <span class="math inline">\(|a_{k,k}^{k-1}|, |a_{k+1,k}^{k-1}|, \ldots |a_{n,k}^{k-1}|\)</span>, then we swap row <span class="math inline">\(k, m\)</span> before elimination.</p>
<p>We can solve any linear equations with <span class="math inline">\(\det A \neq 0\)</span> with column pivoting. Since the total time complexity of comparison is <span class="math inline">\(O(n^2)\)</span>, the time complexity of Gaussian elimination with column pivoting is still <span class="math inline">\(O(n^3)\)</span>.</p>
<h5 id="gauss-jordan-elimination"># Gauss-Jordan Elimination</h5>
<p>Gaussian elimination execute substitution process right after the matrix is converted to upper triangular matrix, while we can do further elimination to convert it into a diagonal matrix, which is called <strong>Gauss-Jordan elimination</strong>.</p>
<h3 id="linear-equation-solving-by-matrix-decomposition">2. Linear Equation Solving by Matrix Decomposition</h3>
<h5 id="relation-between-line-operation-and-matrix-decomposition"># Relation between Line Operation and Matrix Decomposition</h5>
<p>An elementary line operation is equivalent to multiply a elementary matrix. The process of Gaussian elimination is equivalent to multiply a matrix <span class="math inline">\(T\)</span> to <span class="math inline">\(A\)</span>, results in a upper triangular matrix: <span class="math display">\[
TA = U
\]</span></p>
<p>Multiply <span class="math inline">\(L = T^{-1}\)</span> on both sides: <span class="math display">\[
A = LU
\]</span> where <span class="math display">\[
L = T^{-1} =
\begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\]</span> can be proved to be a lower triangular matrix. The form <span class="math inline">\(A = LU\)</span> indicates that we devide the matrix <span class="math inline">\(A\)</span> into the multiplication of two matrixes <span class="math inline">\(L\)</span>, <span class="math inline">\(U\)</span>. where <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> are lower and upper triangular matrix respectively. Equivalently, we can also decompose <span class="math inline">\(A\)</span> as <span class="math inline">\(A=UL\)</span>.</p>
<p>Since you can move the coefficients from one matrix to another, there are infinite decompositions of form <span class="math inline">\(A=LU\)</span>. If <span class="math inline">\(L\)</span> is contrainted to be the unit lower triangular matrix, that unique decomposition is called <strong>Doolittle decomposition</strong>. If <span class="math inline">\(U\)</span> is contrainted to be the unit upper triangle matrix, that unique decomposition is called <strong>Crout decomposition</strong>.</p>
<p>Besides, there are other forms of decomposition such as <span class="math inline">\(A = LDM^T\)</span>, where the <span class="math inline">\(L, M\)</span> is lower triangular matrix (hence <span class="math inline">\(M^T\)</span> is upper triangular matrix), and <span class="math inline">\(D\)</span> is diagonal matrix. Specially, if <span class="math inline">\(A\)</span> is a positive definite matrix, we have <span class="math inline">\(M = L\)</span>, <span class="math inline">\(A = LDL^T\)</span>, this is what we called <strong><span class="math inline">\(LDL^T\)</span> decomposition</strong> or <strong>refined Cholesky decomposition</strong>. Let <span class="math inline">\(P = L\sqrt D\)</span> we have <span class="math inline">\(A = PP^T\)</span>, which is called <strong>Cholesky decomposition</strong>.</p>
<p>The matrix is decomposed into two or three triangular/diagonal matrixes after decomposition, which can be easily solved by substitution. Note that decomposition process is independent of <span class="math inline">\(\bb\)</span>. Hence if there are multiple equations with same <span class="math inline">\(A\)</span> but various <span class="math inline">\(\bb\)</span>, this approach reduces the time complexity by cancelling the repetative processing of <span class="math inline">\(A\)</span>.</p>
<h5 id="lu-decomposition"># LU Decomposition</h5>
<p>Compare elements to determine the matrix after Doolittle decomposition:</p>
<p><span class="math display">\[
A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp;\cdots&amp; a_{1n}  \\
a_{21} &amp; a_{22} &amp;\cdots&amp; a_{2n}  \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp;\cdots&amp; a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp; u_{12} &amp;\cdots &amp; u_{1n}  \\
       &amp; u_{22} &amp;\cdots &amp; u_{2n}  \\
       &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
a_{11} = \sum_{r=1}^n l_{1r}u_{r1} =
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{11} \\
0 \\
\vdots \\
0
\end{bmatrix}
= \sum_{r=1}^1 l_{1r}u_{r1} = u_{11}
\]</span></p>
<p><span class="math display">\[
a_{1j} = \sum_{r=1}^n l_{1r}u_{rj} =
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{1j} \\
u_{2j} \\
\vdots \\
0
\end{bmatrix}
= \sum_{r=1}^1 l_{1r}u_{rj} = u_{1j}
\]</span> Hence <span class="math inline">\(a_{ij} = u_{1j}\)</span>, <span class="math inline">\(j=1,2, \dots, n\)</span>.</p>
<p><span class="math display">\[
\begin{align}
a_{kj}
&amp;= \sum_{r=1}^n l_{kr}u_{rj} =
\begin{bmatrix}
l_{k1} &amp; l_{k2} &amp; \cdots &amp; l_{k,k-1} &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{1j} \\
\vdots \\
u_{jj} \\
0 \\
\vdots \\
0
\end{bmatrix} \\
&amp;= \sum_{r=1}^n l_{kr}u_{rj}
= \sum_{r=1}^{k-1} l_{kr}u_{rj} + u_{kj}
\end{align}
\]</span></p>
<p>Now we have the algorithm for Doolittle decomposition:</p>
<p><img src="__img__/Doolittle_decomposition_algorithm.png" alt="Doolittle decomposition algorithm" style="zoom:30%;" /></p>
<p>Refer to <a href="https://github.com/revectores/revector">https://github.com/revectores/revector</a> for the C++ implementation of Doolittle decomposition.</p>
<p>Refer to <a href="https://github.com/revectores/LU-decomposition-impls">https://github.com/revectores/LU-decomposition-impls</a> for some parallelized Doolittle decomposition implemented in C.</p>
<p>Similarly, the algorithm of Crout decomposition:</p>
<p><img src="__img__/Crout_decomposition_algorithm.png" alt="Doolittle decomposition algorithm" style="zoom:30%;" /></p>
<p>Refer to <a href="https://github.com/revectores/revector">https://github.com/revectores/revector</a> for the C++ implementation of Crout decomposition.</p>
<h5 id="decomposition-of-positive-definite-matrix"># Decomposition of Positive Definite Matrix</h5>
<p>As mentioned above, we have Cholesky decomposition <span class="math inline">\(A = LL^T\)</span> for positive definitive matrix</p>
<p><img src="__img__/Cholesky_decomposition_algorithm.png" alt="Cholesky_decomposition_algorithm" style="zoom:30%;" /></p>
<p>In practice we often decompose as form <span class="math inline">\(A = LDL^T\)</span> instaed to remove square root computation.</p>
<p><img src="__img__/LDLT_decomposition_algorithm.png" alt="LDLT_decomposition_algorithm" style="zoom:30%;" /></p>
<blockquote>
<p><strong>Proof</strong>. To prove the existence of <span class="math inline">\(LDL^T\)</span> decomposition, we first apply Doolittle decomposition to matrix and extract the diagonal line of <span class="math inline">\(U\)</span>: <span class="math display">\[
\begin{align}
A = LU
&amp;= \begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp; u_{12} &amp;\cdots &amp; u_{1n}  \\
       &amp; u_{22} &amp;\cdots &amp; u_{2n}  \\
     &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp;        &amp;         &amp;  \\
       &amp; u_{22} &amp;     &amp; \\
     &amp;        &amp;\ddots &amp; \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix}
\begin{bmatrix}
\o u_{11} &amp; \o u_{12} &amp;\cdots &amp; \o u_{1n}  \\
          &amp; \o u_{22} &amp;\cdots &amp; \o u_{2n}  \\
        &amp;           &amp;\ddots &amp; \vdots \\
          &amp;           &amp;       &amp; \o u_{nn}
\end{bmatrix}
\end{align}
\]</span></p>
<p>Since <span class="math inline">\(A\)</span> is symmetric positive definite matrix, we have <span class="math inline">\(u_{ii}&gt;0\)</span>. We can prove <span class="math inline">\(L = \o U\)</span> since <span class="math display">\[
A = LU = LD\o U^T = A^T = \o U(DL^T)
\]</span> That is, <span class="math display">\[
\begin{align}
A = LDL^T
= \begin{bmatrix}
1 \\
l_{21} &amp; 1  \\
\vdots &amp; \vdots &amp;\ddots \\
l_{n1} &amp; l_{n2} &amp; \cdots &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{11} &amp;      &amp;       &amp;  \\
       &amp; u_{22} &amp;     &amp; \\
     &amp;        &amp;\ddots &amp; \\
       &amp;        &amp;       &amp; u_{nn}
\end{bmatrix}
\begin{bmatrix}
1        &amp; l_{21} &amp;\cdots &amp; l_{n1}  \\
       &amp; 1        &amp;\cdots &amp; l_{n2}  \\
     &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; 1
\end{bmatrix}
\end{align}
\]</span></p>
</blockquote>
<h5 id="condition-number-of-matrix"># Condition Number of Matrix</h5>
<p>For the non-singular matrix <span class="math inline">\(A\)</span>, we define <span class="math display">\[
\kappa_p(A) = \norm{A}_p \norm{A^{-1}}_p
\]</span> as the <strong>condition number</strong> of matrix <span class="math inline">\(A\)</span>. Due to the equivalence of different norms, the condition numbers with different norms are also equivalent.</p>
<p>How the error <span class="math inline">\(\delta x\)</span> is influenced by small disturbance <span class="math inline">\(\delta A\)</span> in coefficient matrix <span class="math inline">\(A\)</span> is represented as <span class="math display">\[
\frac{\norm {\delta x}}{\norm x} \le \frac{\kappa_A \dfrac{\norm{\delta A}}{\norm A}}{1 - \kappa_A\dfrac{\norm{\delta A}}{\norm A}}
\]</span> How the error <span class="math inline">\(\delta x\)</span> is influenced by small disturbance <span class="math inline">\(\delta \bb\)</span> in constant vector <span class="math inline">\(\bb\)</span> is represented as <span class="math display">\[
\frac{\norm {\delta x}}{\norm x} \le \kappa_A\frac{\norm {\delta\bb}}{\norm\bb}
\]</span> We call the matrix with large <span class="math inline">\(\kappa_A\)</span> <strong>ill-conditioned</strong>, which states “the small disturbance causes a huge difference in solution”.</p>
<h3 id="linear-equation-solving-by-iteration">3. Linear Equation Solving by Iteration</h3>
<p>Transform the equations <span class="math inline">\(AX = \by\)</span> into <span class="math inline">\(X = MX + \bg\)</span>, for any <span class="math inline">\(X^{(0)}\in R^n\)</span>, construct the iteration <span class="math display">\[
X^{(k+1)} = MX^{(k)} + \bg
\]</span> if the iteration series <span class="math inline">\(\{X^{(k)}\}\)</span> converges, the limit of iteration series <span class="math inline">\(X^*\)</span> is the solution of equations <span class="math inline">\(AX=\by\)</span>.</p>
<p>In practice, the iteration is terminated once we have <span class="math inline">\(\norm{X^{(k+1)} - X^{(k)}}_p &lt; \varepsilon\)</span> for some pre-chosen error bound <span class="math inline">\(\varepsilon\)</span>, and pick <span class="math inline">\(X^{(k+1)}\)</span> as the approximation of solution.</p>
<p>The convergence of iteration is determined by spectral radius of iteration matrix <span class="math inline">\(M\)</span>. Specifically, the iteration converges if and only if the spectral radius <span class="math inline">\(\rho(M) &lt; 1\)</span>.</p>
<blockquote>
<p><strong>Proof</strong>. If <span class="math inline">\(X^*\)</span> is the solution of equation <span class="math inline">\(AX=\by\)</span>, then <span class="math display">\[
X^* = MX^* + \bg
\]</span></p>
<p><span class="math display">\[
\begin{align}
X^* - X^{(k+1)}
&amp;= M(X^*-X^{k}) \\
&amp;= M^2(X^*-X^{k-1}) \\
&amp;= \cdots \\
&amp;= M^{k+1}(X^*-X^{(0)})
\end{align}
\]</span></p>
<p><span class="math inline">\(\displaystyle \lim_{k\rightarrow \infty} M^{k} = 0\)</span> if and only if <span class="math inline">\(\rho(M) &lt; 1\)</span>. Hence we define the matrix with spectral radius less than 1 <strong>convergent matrix</strong>.</p>
<p>That is, whether the linear equations converges depends on the property of iteration matrix, regardless of the solution <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(X^{(0)}\)</span>.</p>
</blockquote>
<p>By definition, we have to compute all the eigenvalues to get the spectral radius of matrix. We may simplify this work by computing the norm <span class="math inline">\(\norm A_p\)</span>. Note that <span class="math inline">\(\norm A_p \ge \rho(A)\)</span>, if <span class="math inline">\(\norm A_p &lt; 1\)</span>, the iteration matrix must be convergent.</p>
<h5 id="jacobi-iteration"># Jacobi Iteration</h5>
<p><span class="math display">\[
\left\{\begin{array}{ll}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = y_1  \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = y_2  \\
\cdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = y_n  \\
\end{array}\right.
\]</span></p>
<p><span class="math display">\[
\left\{\begin{array}{ll}
x_1 = \dfrac{1}{a_{11}}\left({-a_{12}x_2 - \cdots - a_{1n}x_n + y_1}\right)  \\
x_2 = \dfrac{1}{a_{22}}\left({-a_{21}x_2 - \cdots - a_{2n}x_n + y_2}\right)  \\
\cdots \\
x_n = \dfrac{1}{a_{nn}}\left({-a_{n1}x_2 - \cdots - a_{nn}x_n + y_n}\right)  \\
\end{array}\right.
\]</span></p>
<p>The iteration form <span class="math display">\[
\left\{\begin{array}{ll}
x_1^{(k+1)} = \dfrac{1}{a_{11}}\left({-a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)} + y_1}\right)  \\
x_2^{(k+1)} = \dfrac{1}{a_{22}}\left({-a_{21}x_2^{(k)} - \cdots - a_{2n}x_n^{(k)} + y_2}\right)  \\
\cdots \\
x_n^{(k+1)} = \dfrac{1}{a_{nn}}\left({-a_{n1}x_1^{k} - \cdots - a_{n, n-1}x_n^{(k)} + y_n}\right)  \\
\end{array}\right.
\]</span> The matrix form of Jacobi iteration is <span class="math display">\[
X^{(k+1)} = BX^{k} + \bg
\]</span></p>
<p>where <span class="math inline">\(B = I - D^{-1}A,~\bg = D^{-1}y\)</span>.</p>
<p>There is a shortcut to judge whether Jacobi iteration converges: if the coefficient matrix <span class="math inline">\(A\)</span> meets one of following condition:</p>
<ul>
<li><span class="math inline">\(|a_{ii}| &gt; \displaystyle \sum_{j=1\\j\neq i}^n |a_{ij}|,\quad i=1, 2, \ldots, n\)</span>.</li>
<li><span class="math inline">\(|a_{jj}| &gt; \displaystyle \sum_{i=1\\i\neq j}^n |a_{ij}|,\quad j=1, 2, \ldots, n\)</span></li>
</ul>
<p>The Jacobi iteration must converge. This can be proved by showing the spectral radius of iteration matrix is less than 1.</p>
<p>==TODO: Add the proofs.==</p>
<h5 id="gauss-seidel-iteration"># Gauss-Seidel Iteration</h5>
<p>The Gauss-Seidel iteration use those new values computed in current iteration instead of last one: <span class="math display">\[
\left\{\begin{array}{ll}
x_1^{(k+1)} = \dfrac{1}{a_{11}}\left({-a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)} + y_1}\right)  \\
x_2^{(k+1)} = \dfrac{1}{a_{22}}\left({-a_{21}x_2^{(k+1)} - \cdots - a_{2n}x_n^{(k)} + y_2}\right)  \\
\cdots \\
x_n^{(k+1)} = \dfrac{1}{a_{nn}}\left({-a_{n1}x_1^{(k+1)} - \cdots - a_{n, n-1}x_{n-1}^{(k+1)} + y_n}\right)  \\
\end{array}\right.
\]</span> Denote <span class="math display">\[
D =
\begin{bmatrix}
a_{11}    \\
&amp; a_{22}  \\
&amp;&amp; \ddots \\
&amp;&amp;&amp; a_{nn}
\end{bmatrix},
L =
\begin{bmatrix}
0 \\
a_{21} &amp; 0  \\
\vdots &amp; \vdots &amp;\ddots \\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; 0
\end{bmatrix},
U =
\begin{bmatrix}
0      &amp; a_{12} &amp;\cdots &amp; a_{1n}  \\
       &amp; 0      &amp;\cdots &amp; a_{2n}  \\
       &amp;        &amp;\ddots &amp; \vdots \\
       &amp;        &amp;       &amp; 0
\end{bmatrix}
\]</span> We have <span class="math display">\[
AX = (D+L+U)X = (D+L)X + UX = \by
\]</span> That is, <span class="math display">\[
(D+L)X = -UX + \by
\]</span> Hence <span class="math display">\[
X^{(k+1)} = -(D+L)^{-1}UX^{(k)} + (D+L)^{-1}\by
\]</span> Denote <span class="math inline">\(S = -(D+L)^{-1}U, \b f = (D+L)^{-1}\by\)</span>, the Gauss-Seidel iteration can be expressed by <span class="math display">\[
X^{k+1} = SX^{k} + \b f
\]</span></p>
<h5 id="successive-over-relaxation"># Successive Over-Relaxation</h5>
<p>The SOR method is a variant of Gauss-Seidel method resulting in faster convergence: <span class="math display">\[
\left\{\begin{array}{ll}
x_1^{(k+1)} = (1-\omega)x_1^{(k)} + \omega(b_{12}x_2^{k} + \cdots + b_{1n}x_n^{(k)} + g_1) \\
x_2^{(k+1)} = (1-\omega)x_2^{(k)} + \omega(b_{21}x_2^{k} + \cdots + b_{2n}x_n^{(k)} + g_2) \\
\cdots \\
x_2^{(k+1)} = (1-\omega)x_n^{(k)} + \omega(b_{n1}x_2^{(k+1)} + \cdots + b_{n, n-1}x_{n-1}^{(k+1)} + g_n) \\
\end{array}\right.
\]</span> where <span class="math inline">\(\omega\)</span> is <strong>relaxation factor</strong>.</p>
<p>The matrix form of SOR is <span class="math inline">\(X^{(k+1)} = S_\omega X^{(k)} + \bf\)</span>, where <span class="math display">\[
\begin{align}
S_w &amp;= (I + \omega D^{-1}L)^{-1}[(1-\omega)I - \omega D^{-1}U] \\
f   &amp;= \omega(I + \omega D^{-1}L)Y
\end{align}
\]</span></p>
<blockquote>
<p><strong>Proof</strong>. <span class="math display">\[
\begin{align}
X^{(k+1)} &amp;= (1-\omega)X^{(k)} + \omega(\widetilde LX^{(k+1)} + \widetilde UX^{(k)} + \bg) \\
(I - \omega \widetilde L)X^{(k+1)} &amp;= ((1-\omega)I + \omega \widetilde U)X^{(k)} + \omega\bg \\
X^{(k+1)} &amp;= (I-\omega \wL)^{-1}((1-\omega)I + \omega \wU)X^{(k)} + (I - \omega\wL)^{-1}\omega \bg
\end{align}
\]</span> where <span class="math inline">\(\wL = -D^{-1}L, \wU = -D^{-1}U\)</span>.</p>
</blockquote>
<p>The necessary condition that SOR converges is <span class="math inline">\(0 &lt; \omega &lt; 2\)</span>. Specially, if <span class="math inline">\(A\)</span> is the positive determined matrix, <span class="math inline">\(0 &lt; \omega &lt; 2\)</span> is also the sufficient condition.</p>
<p>The iteration is called</p>
<ul>
<li><strong>Under-relaxation</strong> iteration if <span class="math inline">\(\omega \in (0, 1)\)</span>.</li>
<li>Gauss-Seidel iteration if <span class="math inline">\(\omega = 1\)</span>.</li>
<li><strong>Over-relaxation</strong> iteration if <span class="math inline">\(\omega \in (1, 2)\)</span>.</li>
</ul>
<p>The best <span class="math inline">\(\omega\)</span> (with fastest convergence speed) is hard to determine for specific equation.</p>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>