<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title> Eigenvalues
 </title>
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/newsprint.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/blog.css"> 
  <link rel="stylesheet" type="text/css" href="http://revectores.com/static/css/code.css"> 
</head>
<body>

<p><a href="../"><< numerical_analysis</a></p>

<h1 id="eigenvalues">Eigenvalues</h1>
<p><span class="math display">\[
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\infnorm}[1]{\norm{#1}_{\infty}}
\newcommand{\b}{\boldsymbol}
\newcommand{\bx}{\b x}
\newcommand{\by}{\b y}
\newcommand{\bb}{\b b}
\newcommand{\bg}{\b g}
\newcommand{\bv}{\b v}
\newcommand{\w}{\widetilde}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\o}{\overline}
\]</span></p>
<h3 id="introduction">1. Introduction</h3>
<p>Recall that for matix <span class="math inline">\(A\)</span>, if <span class="math inline">\(A\bv = \lambda \bv\)</span>, we define <span class="math inline">\(\lambda\)</span> as the <strong>eigenvalue</strong> and <span class="math inline">\(\bv\)</span> as the corresponding <strong>eigenvector</strong> of <span class="math inline">\(A\)</span>.</p>
<p>To compute the eigenvalues and eigenvectors by definition, we need to solve equation <span class="math inline">\(|\lambda I - A| = 0\)</span>. The order of this equation is at most the order of matrix. For instance, to compute the eigenvalue of <span class="math inline">\(A = \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}\)</span>, we must solve the euqation <span class="math display">\[
\begin{vmatrix}
\lambda &amp; 1 \\
1 &amp; \lambda - 1
\end{vmatrix} = \lambda(\lambda-1) - 1 = 0
\]</span> However, <a href="https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem">Abel–Ruffini theorem</a> states that there is no solution in radicals to general polynomial equations of degree five or higher with arbitrary coefficients. Hence computing the eigenvalue/eigenvector by definition is not an universal approach.</p>
<p>The <strong>dominant eigenvalue</strong>, or <strong>largest magnitude eigenvalue</strong> of <span class="math inline">\(A\)</span> is an eigenvalue <span class="math inline">\(\lambda\)</span> whose magnitude is greater than all other eigenvalues. If it exists, an eigenvector associated to <span class="math inline">\(\lambda\)</span> is called a <strong>dominant eigenvector</strong>.</p>
<p>The dominant eigenvalues is much more significant than others in many practical problems. For instance, whether the iteration matrix converges when solving linear equations depends on the spectral radius (a.k.a. dominant eigenvalue) of matix.</p>
<p><strong>Power method</strong> is one of the numerical technique to compute dominant eigenvalue, and based on which we developed <strong>inverse power method</strong> to compute smallest magnitude eigenvalue.</p>
<h3 id="power-method">2. Power Method</h3>
<h5 id="power-method-introduction"># Power Method Introduction</h5>
<p>Any vector <span class="math inline">\(X^{(0)}\)</span> (as the initial vector) can be expressed by <span class="math inline">\(n\)</span> linear independent eigenvector of <span class="math inline">\(A\)</span> <span class="math display">\[
X^{(0)} = \alpha_1\bv_1 + \alpha_2 \bv_2 + \cdots + \alpha_n \bv_n
\]</span></p>
<p>And the multiplication <span class="math inline">\(AX^{(0)}\)</span> can be expressed by <span class="math display">\[
\begin{align}
X^{(1)}
&amp;= AX^{(0)} \\
&amp;= A(\alpha_1 \bv_1 + \alpha_2 \bv_2 + \cdots + \alpha_n \bv_n) \\
&amp;= A\alpha_1 \bv_1 + A\alpha_2 \bv_2 + \cdots + A\alpha_n \bv_n \\
&amp;= \lambda_1\alpha_1 \bv_1 + \lambda_2\alpha_2 \bv_2 + \cdots + \lambda_n\alpha_n \bv_n
\end{align}
\]</span></p>
<p>Applying mathematical induction , we have <span class="math display">\[
X^{(k)}
= AX^{(k-1)}
= \lambda_1^k\alpha_1 \bv_1 + \lambda_2^k\alpha_2 \bv_2 + \cdots + \lambda_n^k\alpha_n \bv_n
\]</span> This expression indicates that how <span class="math inline">\(X^{(k)}\)</span> change when iterating depends on the distribution of eigenvalues. We’ll discuss two simple cases.</p>
<h5 id="two-cases-when-applying-power-method"># Two Cases when Applying Power Method</h5>
<p><strong>Case 1</strong>. If the ratio <span class="math inline">\(\dfrac{x^{(k+1)}}{x^{(k)}}\)</span> converges, the largest magnitude eigenvalue is unique and is a simple root. The dominant eigenvalue and the dominant eigenvector can be computed by <span class="math display">\[
\left\{\begin{array}{ll}
\lambda_1 \approx \dfrac{x_i^{(k+1)}}{x_i^{(k)}}, \quad i = 1, 2, \ldots, n \\
\bv_1 \approx X^{(k)}
\end{array}\right.
\]</span></p>
<blockquote>
<p><strong>Proof</strong>. For the unique dominant eigenvalue <span class="math inline">\(\lambda_1\)</span>, <span class="math display">\[
\begin{align}
X^{(k)}
&amp;= \lambda_1^k\alpha_1 \bv_1 + \lambda_2^k\alpha_2 \bv_2 + \cdots + \lambda_n^k\alpha_n \bv_n \\
&amp;= \lambda_1^k\left[ \lambda_1^k\alpha_1 \bv_1 + \left(\frac{\lambda_2}{\lambda_1}\right)^k\alpha_2 \bv_2 + \cdots + \left(\frac{\lambda_n}{\lambda_1}\right)^k\alpha_n \bv_n \right]
\end{align}
\]</span> For sufficiently large <span class="math inline">\(k\)</span>, we have <span class="math inline">\(\left| \left( \dfrac{\lambda_i}{\lambda_1} \right)^k \right| \rightarrow 0\)</span> for <span class="math inline">\(i \neq 1\)</span> since <span class="math inline">\(\lambda_1\)</span> is the dominant eigenvalue. Hence <span class="math display">\[
\begin{align}
X^{(k)}   &amp;\approx \lambda_1^k\alpha_1\bv_1 \\
X^{(k+1)} &amp;\approx \lambda_1^{k+1}\alpha_1\bv_1 = \lambda_1 \lambda^k\alpha_1\bv_1 = \lambda_1X^{(k)}
\end{align}
\]</span></p>
</blockquote>
<p><strong>Case 2</strong>. If the ratio <span class="math inline">\(\dfrac{x^{(k+1)}}{x^{(k)}}\)</span> does not converges but <span class="math inline">\(\dfrac{x^{(2k+2)}}{x^{(2k)}}\)</span> does, then dominant eigenvalues are the real roots with different signs. The dominant eigenvalue and eigenvector can be computed by <span class="math display">\[
\left\{\begin{array}{ll}
\lambda_1 = \sqrt{\dfrac{x_1^{(k+2)}}{x_i^{(k)}}}, \quad i = 1, 2, \ldots, n \\
\bv_1 = X^{(k+1)} + \lambda_1 X^{(k)} \\
\bv_2 = X^{(k+1)} - \lambda_1 X^{(k)}
\end{array}\right.
\]</span></p>
<blockquote>
<p><strong>Proof</strong>. For two dominant eigenvalues <span class="math inline">\(\lambda_1, \lambda_2\)</span> and <span class="math inline">\(\lambda_1 = -\lambda_2\)</span> <span class="math display">\[
\begin{align}
X^{(k)}
&amp;= \lambda_1^k\alpha_1 \bv_1 + \lambda_2^k\alpha_2 \bv_2 + \cdots + \lambda_n^k\alpha_n \bv_n \\
&amp;=
\lambda_1^k\left[ \lambda_1^k\alpha_1 \bv_1
+ (-1)^k\alpha_2 \bv_2
+ \left(\frac{\lambda_3}{\lambda_1}\right)^k\alpha_3 \bv_n
+ \cdots
+ \left(\frac{\lambda_n}{\lambda_1}\right)^k\alpha_n \bv_n \right]
\end{align}
\]</span> For sufficiently large <span class="math inline">\(k\)</span>, we have <span class="math display">\[
\begin{align}
X^{(k)} &amp;\approx \lambda_1^k(\alpha_1\bv_1 + (-1)^k\alpha_2\bv_2) \\
X^{(k+1)} &amp;\approx \lambda_1^{k+1}(\alpha_1\bv_1 + (-1)^{k+1}\alpha_2\bv_2) \\
X^{(k+2)} &amp;\approx \lambda_1^{k+2}(\alpha_1\bv_1 + (-1)^{k+2}\alpha_2\bv_2) \\
\end{align}
\]</span></p>
</blockquote>
<p>Those trends that do not fit the two cases above require other special processings.</p>
<h5 id="normalized-power-method"># Normalized Power Method</h5>
<p>If the dominant eigenvalue is larger or smaller than <span class="math inline">\(1\)</span>, some components of <span class="math inline">\(X^{(k)}\)</span> will increase or decrease continuously when iterating, which leads to the precision problem due to <a href="">IEEE754</a>. Hence we can normalize the vector after each step: <span class="math display">\[
Y_0 = \frac{X^{(0)}}{\infnorm{X^{(0)}}} \\
\left\{\begin{array}{ll}
X^{(k+1)} = AY^{(k)} \\
Y^{(k+1)} = \dfrac{X^{(k+1)}}{\infnorm{X^{(k+1)}}}
\end{array}\right.
\]</span> The normalization process gurantees <span class="math inline">\(\norm{Y^{(k)}}_\infty = 1\)</span>, that is, the component that represents dominant eigenvalue of <span class="math inline">\(A\)</span> remains <span class="math inline">\(\pm1\)</span>.</p>
<p>There are three cases for normalized power method:</p>
<ol type="1">
<li><p>If <span class="math inline">\(\{X^{k}\}\)</span> converges, there is unique <strong>positive</strong> dominant eigenvalue <span class="math display">\[
\left\{\begin{array}{ll}
\displaystyle \lambda_1 \approx \max_{1\le i \le n} |x_i^{(k)}| = |x_j^{(k)}| \\
\bv_1 \approx Y^{(k)}
\end{array}\right.
\]</span></p></li>
<li><p>If <span class="math inline">\(\{X^{(2k)}\}, \{X^{(2k+1)}\}\)</span> converges to two opposite vectors, there is unique <strong>negative</strong> dominant eigenvalue <span class="math display">\[
\left\{\begin{array}{ll}
\displaystyle \lambda_1 \approx - \max_{1\le i \le n} |x_i^{(k)}| = - |x_j^{(k)}| \\
\bv_1 \approx Y^{(k)}
\end{array}\right.
\]</span></p></li>
<li><p>If <span class="math inline">\({\{X^{(2k)}\}, \{X^{(2k+1)}\}}\)</span> converges to two different vectors, there are two different dominant eigenvectors with opposite sign. For sufficiently large <span class="math inline">\(k\)</span>, we exectue one more <strong>unnormalized</strong> operation <span class="math inline">\(X^{(k+1)} = AX^{(k)} = A^2Y^{(k-1)}\)</span>, then <span class="math display">\[
\left\{\begin{array}{ll}
\lambda_1 \approx \displaystyle\sqrt{\frac{x_i^{(k+1)}}{y_i^{(k-1)}}} \\
\lambda_2 = -\lambda_1 \\
\bv_1 = X^{(k+1)} + \lambda_1 X^{(k)} \\
\bv_2 = X^{(k+1)} - \lambda_1 X^{(k)}
\end{array}\right.
\]</span></p></li>
<li><p>It requires special pocessing if <span class="math inline">\(\{ X^{(k)} \}\)</span> does not fit any cases mentioned above.</p></li>
</ol>
<h3 id="inverse-power-method">3. Inverse Power Method</h3>
<p><strong>Inverse power method</strong> is used to compute the smallest magnitude eigenvalue.</p>
<p>In equation <span class="math display">\[
A\bv = \lambda\bv
\]</span> we multiple <span class="math inline">\(A^{-1}\)</span> on both sides: <span class="math display">\[
A^{-1}\bv = \lambda^{-1}\bv
\]</span> That is, the smallest magnitude eigenvalue of <span class="math inline">\(A\)</span> is exactly the reciprocal of dominant eigenvalue of <span class="math inline">\(A^{-1}\)</span>.</p>
<p>In practice, we do apply power method after computing <span class="math inline">\(A^{-1}\)</span> but keep solving equation <span class="math display">\[
AX^{(k+1)} = X^{(k)}
\]</span> <a href="">decomposition method</a> is applied to decompose <span class="math inline">\(A = LU\)</span> first then we solve <span class="math display">\[
\left\{\begin{array}{ll}
LZ^{(k+1)} = X^{(k)} \\
UX^{(k+1)} = Z^{(k+1)}
\end{array}\right.
\]</span> We can also normalize the inverse power method as well to avoid the precision problem: <span class="math display">\[
\left\{\begin{array}{ll}
Y^{(k)} = \dfrac{X^{(k)}}{\infnorm{X^{(k)}}} \\
AX^{(k+1)} = Y^{(k)}\\
\end{array}\right.
\]</span></p>


</body>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>